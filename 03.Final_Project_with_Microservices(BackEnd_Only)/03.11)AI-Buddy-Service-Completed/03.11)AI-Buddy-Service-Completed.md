# ðŸ“ `src\agent\agent.js`

## âœ… Original code (unchanged)

```js
const { StateGraph, MessagesAnnotation } = require("@langchain/langgraph")
const { ChatGoogleGenerativeAI } = require("@langchain/google-genai")
const { ToolMessage, AIMessage, HumanMessage } = require("@langchain/core/messages")
const tools = require("./tools")    


const model = new ChatGoogleGenerativeAI({
    model: "gemini-2.0-flash",
    temperature: 0.5,
})


const graph = new StateGraph(MessagesAnnotation)
    .addNode("tools", async (state, config) => {

        const lastMessage = state.messages[ state.messages.length - 1 ]

        const toolsCall = lastMessage.tool_calls

        const toolCallResults = await Promise.all(toolsCall.map(async (call) => {

            const tool = tools[ call.name ]
            if (!tool) {
                throw new Error(`Tool ${call.name} not found`)
            }
            const toolInput = call.args

            console.log("Invoking tool:", call.name, "with input:", call)

            const toolResult = await tool.func({ ...toolInput, token: config.metadata.token })

            return new ToolMessage({ content: toolResult, name: call.name })

        }))

        state.messages.push(...toolCallResults)

        return state
    })
    .addNode("chat", async (state, config) => {
        const response = await model.invoke(state.messages, { tools: [ tools.searchProduct, tools.addProductToCart ] })


        state.messages.push(new AIMessage({ content: response.text, tool_calls: response.tool_calls }))

        return state

    })
    .addEdge("__start__", "chat")
    .addConditionalEdges("chat", async (state) => {

        const lastMessage = state.messages[ state.messages.length - 1 ]

        if (lastMessage.tool_calls && lastMessage.tool_calls.length > 0) {
            return "tools"
        } else {
            return "__end__"
        }

    })
    .addEdge("tools", "chat")


const agent = graph.compile()


module.exports = agent
```

---

## ðŸ§  Deep Explanation (line by line)

### Imports

```js
const { StateGraph, MessagesAnnotation } = require("@langchain/langgraph")
```

* **StateGraph**
  Think of this as a **state machine for AI reasoning**.

  * Nodes = steps (chat, tools)
  * Edges = transitions
* **MessagesAnnotation**

  * Defines that your state contains a `messages[]` array
  * Each node receives and returns `{ messages }`

---

```js
const { ChatGoogleGenerativeAI } = require("@langchain/google-genai")
```

* Wrapper around **Google Gemini**
* Handles:

  * Prompt formatting
  * Tool calling
  * Response parsing

---

```js
const { ToolMessage, AIMessage, HumanMessage } = require("@langchain/core/messages")
```

These are **message types** inside LangChain:

| Type           | Who sent it | Purpose                  |
| -------------- | ----------- | ------------------------ |
| `HumanMessage` | User        | User input               |
| `AIMessage`    | LLM         | Normal model output      |
| `ToolMessage`  | Tool        | Result of tool execution |

---

```js
const tools = require("./tools")
```

* Loads your **tool registry**
* Tools are callable by name:

  * `searchProduct`
  * `addProductToCart`

---

### Model creation

```js
const model = new ChatGoogleGenerativeAI({
    model: "gemini-2.0-flash",
    temperature: 0.5,
})
```

* **temperature 0.5**

  * Balanced creativity
  * Predictable tool calling
* Model can:

  * Decide **when to call tools**
  * Generate `tool_calls[]`

---

## ðŸ§© StateGraph construction

```js
const graph = new StateGraph(MessagesAnnotation)
```

* Initial state shape:

```js
{
  messages: []
}
```

---

## ðŸ”§ `tools` node (execution engine)

```js
.addNode("tools", async (state, config) => {
```

* Executes **tool calls generated by the model**
* `config.metadata.token` comes from socket auth

---

```js
const lastMessage = state.messages[state.messages.length - 1]
```

* Tool calls are **always attached to last AI message**

---

```js
const toolsCall = lastMessage.tool_calls
```

* Example:

```json
[
  { "name": "searchProduct", "args": { "query": "iphone" } }
]
```

---

### Execute tools in parallel

```js
await Promise.all(toolsCall.map(async (call) => {
```

* Multiple tool calls supported
* Parallel execution = faster

---

```js
const tool = tools[ call.name ]
```

* Dynamic lookup
* If model hallucinates a tool â†’ crash safely

---

```js
const toolResult = await tool.func({
  ...toolInput,
  token: config.metadata.token
})
```

* Injects **JWT token**
* Enables **secure backend access**

---

```js
return new ToolMessage({
  content: toolResult,
  name: call.name
})
```

* Tool results must be wrapped
* Model reads this as **context**, not user input

---

```js
state.messages.push(...toolCallResults)
return state
```

* Tool output is appended
* Control returns to graph

---

## ðŸ’¬ `chat` node (LLM reasoning)

```js
.addNode("chat", async (state, config) => {
```

* Core AI step
* Decides:

  * Respond normally
  * OR call tools again

---

```js
const response = await model.invoke(
  state.messages,
  { tools: [ tools.searchProduct, tools.addProductToCart ] }
)
```

* Sends:

  * Full conversation
  * Tool definitions
* Gemini decides **if tools are needed**

---

```js
state.messages.push(
  new AIMessage({
    content: response.text,
    tool_calls: response.tool_calls
  })
)
```

* Stores:

  * Natural language response
  * Tool calls (if any)

---

## ðŸ”€ Graph control flow

```js
.addEdge("__start__", "chat")
```

* Entry point â†’ chat

---

```js
.addConditionalEdges("chat", async (state) => {
```

Decision logic:

```js
if (lastMessage.tool_calls.length > 0)
  return "tools"
else
  return "__end__"
```

**Meaning:**

* If tools needed â†’ execute them
* Else â†’ stop graph

---

```js
.addEdge("tools", "chat")
```

* Tool output â†’ model reasoning again
* Enables **multi-step reasoning loop**

---

```js
const agent = graph.compile()
module.exports = agent
```

* Compiles state machine into callable AI agent

---

# ðŸ“ `src\agent\tools.js`

## âœ… Original code

```js
const { tool } = require("@langchain/core/tools")
const { z } = require("zod")
const axios = require("axios")
```

---

### `searchProduct` tool

```js
const searchProduct = tool(async ({ query, token }) => {
```

* `tool()` registers function for LLM
* Input validated via Zod

---

```js
const response = await axios.get(
  `http://nova-alb.../api/products?q=${query}`,
  { headers: { Authorization: `Bearer ${token}` } }
)
```

* Secure backend request
* Uses **user JWT**

---

```js
return JSON.stringify(response.data)
```

* LLM understands JSON better than raw objects

---

```js
schema: z.object({
  query: z.string()
})
```

* Prevents malformed calls
* Model must supply `query`

---

### `addProductToCart` tool

```js
const addProductToCart = tool(async ({ productId, qty = 1, token }) => {
```

* `qty` defaults to 1
* Token injected by agent

---

```js
await axios.post(
  `/api/cart/items`,
  { productId, qty },
  { headers: { Authorization: `Bearer ${token}` } }
)
```

---

```js
return `Added product with id ${productId} (qty: ${qty}) to cart`
```

* Human-readable confirmation

---

```js
module.exports = { searchProduct, addProductToCart }
```

---

# ðŸ“ `src\app.js`

```js
const express = require('express')
const app = express()
```

* Minimal express bootstrap

---

```js
app.get('/', (req, res) => {
  res.status(200).json({ message: "AI service is running" })
})
```

* Health check endpoint
* Used by ALB / monitoring

---

# ðŸ“ `src\sockets\socket.server.js`

## Authentication

```js
io.use((socket, next) => {
```

* Runs **before connection accepted**

---

```js
const cookies = socket.handshake.headers?.cookie
const { token } = cookie.parse(cookies)
```

* Extract JWT from cookies

---

```js
jwt.verify(token, process.env.JWT_SECRET)
```

* Rejects unauthenticated users

---

## Message handling

```js
socket.on('message', async (data) => {
```

* Receives user text

---

```js
const agentResponse = await agent.invoke({
  messages: [{ role: "user", content: data }]
}, {
  metadata: { token: socket.token }
})
```

* Starts graph execution
* Token passed to tools

---

```js
const lastMessage = agentResponse.messages.at(-1)
socket.emit('message', lastMessage.content)
```

* Sends **final AI response only**

---

## ðŸ§  Big Picture Mental Model

```
User â†’ Socket â†’ Agent
      â†“
   Chat Node
      â†“
Tool Needed?
   â†“       â†“
 Tools     End
   â†“
 Chat again
```

This is **production-grade agent orchestration**, not a demo.

---

Here's the complete code with detailed line-by-line explanations:

## **src/agent/agent.js**

```javascript
// Import required modules from LangChain framework
const { StateGraph, MessagesAnnotation } = require("@langchain/langgraph")
const { ChatGoogleGenerativeAI } = require("@langchain/google-genai")
const { ToolMessage, AIMessage, HumanMessage } = require("@langchain/core/messages")
// Import custom tools that will be available to the agent
const tools = require("./tools")    

// Initialize the Google Gemini AI model with specific parameters
// "gemini-2.0-flash" is a fast, efficient model for real-time applications
// Temperature controls randomness: 0.5 = balanced creativity and determinism
const model = new ChatGoogleGenerativeAI({
    model: "gemini-2.0-flash",
    temperature: 0.5,
})

// Create a StateGraph which represents the agent's decision-making flow
// MessagesAnnotation ensures the state maintains a list of message exchanges
const graph = new StateGraph(MessagesAnnotation)
    // Add a node for handling tool execution
    .addNode("tools", async (state, config) => {
        // Get the most recent message from the conversation history
        const lastMessage = state.messages[ state.messages.length - 1 ]
        
        // Extract any tool calls from the AI's response
        const toolsCall = lastMessage.tool_calls
        
        // Process all tool calls in parallel for efficiency
        const toolCallResults = await Promise.all(toolsCall.map(async (call) => {
            // Find the corresponding tool by name
            const tool = tools[ call.name ]
            if (!tool) {
                throw new Error(`Tool ${call.name} not found`)
            }
            // Extract input arguments for the tool
            const toolInput = call.args
            
            // Log tool invocation for debugging and monitoring
            console.log("Invoking tool:", call.name, "with input:", call)
            
            // Execute the tool function with provided arguments
            // config.metadata.token provides user authentication token from socket connection
            const toolResult = await tool.func({ ...toolInput, token: config.metadata.token })
            
            // Create a ToolMessage with the execution result
            return new ToolMessage({ content: toolResult, name: call.name })
        }))
        
        // Append all tool execution results to the conversation history
        state.messages.push(...toolCallResults)
        
        // Return updated state to continue the graph execution
        return state
    })
    // Add a node for AI chat interactions
    .addNode("chat", async (state, config) => {
        // Send conversation history to AI model with available tools
        // Only specific tools are bound: searchProduct and addProductToCart
        const response = await model.invoke(state.messages, { tools: [ tools.searchProduct, tools.addProductToCart ] })
        
        // Add AI's response (which may include new tool calls) to history
        state.messages.push(new AIMessage({ 
            content: response.text, 
            tool_calls: response.tool_calls 
        }))
        
        return state
    })
    // Define the starting point of the graph execution
    .addEdge("__start__", "chat")
    // Add conditional routing based on AI's response
    .addConditionalEdges("chat", async (state) => {
        // Check the most recent message
        const lastMessage = state.messages[ state.messages.length - 1 ]
        
        // If AI requested tool usage, route to tools node
        if (lastMessage.tool_calls && lastMessage.tool_calls.length > 0) {
            return "tools"
        } else {
            // Otherwise, end the conversation
            return "__end__"
        }
    })
    // After tools execute, always return to chat for next AI response
    .addEdge("tools", "chat")

// Compile the graph into an executable agent
const agent = graph.compile()

// Export the agent for use in other parts of the application
module.exports = agent
```

## **src/agent/tools.js**

```javascript
// Import required modules for creating tools and validation
const { tool } = require("@langchain/core/tools")
const { z } = require("zod")  // Schema validation library
const axios = require("axios") // HTTP client for API calls

// Define first tool: searchProduct - searches products in the e-commerce API
const searchProduct = tool(async ({ query, token }) => {
    // Log function call for debugging purposes
    console.log("searchProduct called with data:", { query, token })
    
    // Make GET request to product search endpoint
    // Note: URL should ideally be from environment variables
    const response = await axios.get(
        `http://nova-alb-551701734.ap-northeast-3.elb.amazonaws.com/api/products?q=${query}`, 
        {
            headers: {
                // Pass authentication token for user authorization
                Authorization: `Bearer ${token}`
            }
        }
    )
    
    // Convert API response to JSON string for AI consumption
    return JSON.stringify(response.data)
}, {
    // Tool metadata for AI to understand when/how to use it
    name: "searchProduct",
    description: "Search for products based on a query",
    // Input validation schema using Zod
    schema: z.object({
        query: z.string().describe("The search query for products")
    })
})

// Define second tool: addProductToCart - adds items to shopping cart
const addProductToCart = tool(async ({ productId, qty = 1, token }) => {
    // Make POST request to cart endpoint with product details
    const response = await axios.post(
        `http://nova-alb-551701734.ap-northeast-3.elb.amazonaws.com/api/cart/items`,
        {
            productId,  // Product identifier
            qty         // Quantity (defaults to 1)
        },
        {
            headers: {
                // Authentication header for user-specific cart
                Authorization: `Bearer ${token}`
            }
        }
    )
    
    // Return human-readable success message
    return `Added product with id ${productId} (qty: ${qty}) to cart`
}, {
    // Tool metadata
    name: "addProductToCart",
    description: "Add a product to the shopping cart",
    // Input validation schema
    schema: z.object({
        productId: z.string().describe("The id of the product to add to the cart"),
        qty: z.number().describe("The quantity of the product to add to the cart").default(1),
    })
})

// Export both tools for use in the agent
module.exports = { searchProduct, addProductToCart }
```

## **src/app.js**

```javascript
// Import Express framework for creating HTTP server
const express = require('express');

// Create Express application instance
const app = express();

// Define root endpoint for health checks and status monitoring
app.get('/', (req, res) => {
    // Return JSON response indicating service is operational
    res.status(200).json({
        message: "AI service is running"
    });
});

// Export the Express app for use in server setup
module.exports = app;
```

## **src/sockets/socket.server.js**

```javascript
// Import required modules for WebSocket functionality
const { Server } = require('socket.io');          // Socket.IO server
const jwt = require('jsonwebtoken');              // JWT token verification
const cookie = require('cookie');                 // Cookie parsing utility
const agent = require('../agent/agent');          // AI agent from previous file

// Initialize Socket.IO server with configuration
async function initSocketServer(httpServer) {
    // Create Socket.IO server instance
    const io = new Server(httpServer, {
        // Define custom path for Socket.IO endpoints
        path: "/api/socket/socket.io/",
    })

    // Middleware for authenticating socket connections
    io.use((socket, next) => {
        // Extract cookies from handshake headers
        const cookies = socket.handshake.headers?.cookie;
        
        // Parse cookies to find JWT token
        const { token } = cookies ? cookie.parse(cookies) : {};
        
        // Reject connection if no token provided
        if (!token) {
            return next(new Error('Token not provided'));
        }

        try {
            // Verify JWT token using secret from environment variables
            const decoded = jwt.verify(token, process.env.JWT_SECRET);
            
            // Attach user information and token to socket for later use
            socket.user = decoded;  // Decoded JWT payload (user data)
            socket.token = token;   // Raw token string
            
            // Allow connection to proceed
            next()
        } catch (err) {
            // Reject connection if token is invalid or expired
            next(new Error('Invalid token'));
        }
    })

    // Event handler for new client connections
    io.on('connection', (socket) => {
        // Log connection details (user info and token)
        console.log(socket.user, socket.token)

        // Handle incoming messages from client
        socket.on('message', async (data) => {
            // Process message through AI agent
            const agentResponse = await agent.invoke({
                messages: [
                    {
                        role: "user",      // Message role identifier
                        content: data      // User's message content
                    }
                ]
            }, {
                metadata: {
                    // Pass authentication token to agent for API calls
                    token: socket.token
                }
            })

            // Extract the final response from agent
            const lastMessage = agentResponse.messages[ agentResponse.messages.length - 1 ]
            
            // Send AI response back to client via socket
            socket.emit('message', lastMessage.content)
        })
    })
}

// Export the initialization function for use in main server file
module.exports = { initSocketServer };
```

## **Key Architecture Notes:**

1. **Agent Flow**: User â†’ Socket â†’ Agent â†’ (Tools â†’ API) â†’ Agent â†’ Socket â†’ User
2. **Authentication**: JWT tokens passed via cookies for secure communication
3. **Tool System**: Modular design allows easy addition of new capabilities
4. **State Management**: LangGraph maintains conversation context across tool calls
5. **Error Handling**: Basic error handling with potential for enhancement
6. **Scalability**: Stateless design supports horizontal scaling

## **Potential Improvements:**
- Environment variables for API URLs
- More robust error handling
- Input sanitization and validation
- Rate limiting and security headers
- Comprehensive logging system
- Response caching for frequent queries